{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zemyPKg4YAF4"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models\n","from torchvision.models.vgg import VGG\n","# from BagData import dataloader\n","import pdb\n","import numpy as np\n","import time\n","# import visdom\n","import numpy as np\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.utils.data import DataLoader\n","\n","\n","class FCN32s(nn.Module):\n","\n","    def __init__(self, pretrained_net, n_class):\n","        super().__init__()\n","        self.n_class = n_class\n","        self.pretrained_net = pretrained_net\n","        self.relu    = nn.ReLU(inplace=True)\n","        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn1     = nn.BatchNorm2d(512)\n","        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn2     = nn.BatchNorm2d(256)\n","        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn3     = nn.BatchNorm2d(128)\n","        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn4     = nn.BatchNorm2d(64)\n","        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn5     = nn.BatchNorm2d(32)\n","        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n","\n","    def forward(self, x):\n","        output = self.pretrained_net(x)\n","        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n","\n","        score = self.bn1(self.relu(self.deconv1(x5)))     # size=(N, 512, x.H/16, x.W/16)\n","        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n","        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n","        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n","        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n","        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n","\n","        return score  # size=(N, n_class, x.H/1, x.W/1)\n","\n","\n","class FCN16s(nn.Module):\n","\n","    def __init__(self, pretrained_net, n_class):\n","        super().__init__()\n","        self.n_class = n_class\n","        self.pretrained_net = pretrained_net\n","        self.relu    = nn.ReLU(inplace=True)\n","        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn1     = nn.BatchNorm2d(512)\n","        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn2     = nn.BatchNorm2d(256)\n","        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn3     = nn.BatchNorm2d(128)\n","        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn4     = nn.BatchNorm2d(64)\n","        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn5     = nn.BatchNorm2d(32)\n","        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n","\n","    def forward(self, x):\n","        output = self.pretrained_net(x)\n","        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n","        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n","\n","        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n","        score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)\n","        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n","        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n","        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n","        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n","        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n","\n","        return score  # size=(N, n_class, x.H/1, x.W/1)\n","\n","\n","\n","class FCNs(nn.Module):\n","\n","    def __init__(self, pretrained_net, n_class):\n","        super().__init__()\n","        self.n_class = n_class\n","        self.pretrained_net = pretrained_net\n","        self.relu    = nn.ReLU(inplace=True)\n","        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn1     = nn.BatchNorm2d(512)\n","        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn2     = nn.BatchNorm2d(256)\n","        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn3     = nn.BatchNorm2d(128)\n","        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn4     = nn.BatchNorm2d(64)\n","        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn5     = nn.BatchNorm2d(32)\n","        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n","\n","    def forward(self, x):\n","        output = self.pretrained_net(x)\n","        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n","        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n","        x3 = output['x3']  # size=(N, 256, x.H/8,  x.W/8)\n","        x2 = output['x2']  # size=(N, 128, x.H/4,  x.W/4)\n","        x1 = output['x1']  # size=(N, 64, x.H/2,  x.W/2)\n","\n","        score = self.bn1(self.relu(self.deconv1(x5)))     # size=(N, 512, x.H/16, x.W/16)\n","        score = score + x4                                # element-wise add, size=(N, 512, x.H/16, x.W/16)\n","        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n","        score = score + x3                                # element-wise add, size=(N, 256, x.H/8, x.W/8)\n","        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n","        score = score + x2                                # element-wise add, size=(N, 128, x.H/4, x.W/4)\n","        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n","        score = score + x1                                # element-wise add, size=(N, 64, x.H/2, x.W/2)\n","        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n","        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n","\n","        return score  # size=(N, n_class, x.H/1, x.W/1)\n","\n","\n","class VGGNet(VGG):\n","    def __init__(self, pretrained=True, model='vgg16', requires_grad=True, remove_fc=True, show_params=False):\n","        super().__init__(make_layers(cfg[model]))\n","        self.ranges = ranges[model]\n","\n","        if pretrained:\n","            exec(\"self.load_state_dict(models.%s(pretrained=True).state_dict())\" % model)\n","\n","        if not requires_grad:\n","            for param in super().parameters():\n","                param.requires_grad = False\n","\n","        if remove_fc:  # delete redundant fully-connected layer params, can save memory\n","            del self.classifier\n","\n","        if show_params:\n","            for name, param in self.named_parameters():\n","                print(name, param.size())\n","\n","    def forward(self, x):\n","        output = {}\n","        # get the output of each maxpooling layer (5 maxpool in VGG net)\n","        for idx in range(len(self.ranges)):\n","            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):\n","                x = self.features[layer](x)\n","            output[\"x%d\"%(idx+1)] = x\n","\n","        return output\n","\n","\n","ranges = {\n","    'vgg11': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),\n","    'vgg13': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),\n","    'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),\n","    'vgg19': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))\n","}\n","\n","# cropped version from https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n","cfg = {\n","    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","\n","def make_layers(cfg, batch_norm=False):\n","    layers = []\n","    in_channels = 3\n","    for v in cfg:\n","        if v == 'M':\n","            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","        else:\n","            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","            if batch_norm:\n","                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n","            else:\n","                layers += [conv2d, nn.ReLU(inplace=True)]\n","            in_channels = v\n","    return nn.Sequential(*layers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mb2TuG90nNiv"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":293920,"status":"ok","timestamp":1692147206077,"user":{"displayName":"이상범","userId":"04534362610835741286"},"user_tz":-540},"id":"cFz-9DDQeYUA","outputId":"7ecde3e2-8b8d-456d-ae0f-535f669dbfc3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to /data/VOCtrainval_11-May-2012.tar\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1999639040/1999639040 [01:58<00:00, 16900698.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting /data/VOCtrainval_11-May-2012.tar to /data\n","Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to ./data/VOCtrainval_11-May-2012.tar\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1999639040/1999639040 [01:58<00:00, 16860510.32it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n","Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n","Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n"]}],"source":["# 변형 설정\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((256, 256))\n","    # 다음 변형은 마스크 이미지도 텐서로 변환하고 클래스를 인덱스로 표현\n","    # transforms.Lambda(lambda x: x * 255.0),  # 마스크 이미지 값을 [0, 255] 범위로 변환\n","])\n","\n","# PASCAL VOC 2012 Segmentation 데이터셋 불러오기\n","train_dataset = datasets.VOCSegmentation(root='/data', year='2012', image_set='train', transform=transform, target_transform=transform, download=True)\n","test_dataset = datasets.VOCSegmentation(root='./data', year='2012', image_set='val', transform=transform, download=True)\n","val_dataset = datasets.VOCSegmentation(root='./data', year='2012', image_set='trainval', transform=transform, download=True)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10908,"status":"ok","timestamp":1692147216979,"user":{"displayName":"이상범","userId":"04534362610835741286"},"user_tz":-540},"id":"QhUf2GKEk9BG","outputId":"eab9db40-bb71-46c3-fbc2-487174b0e396"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:02<00:00, 248MB/s]\n"]}],"source":["VGG = VGGNet()\n","model = FCNs(VGG, 21).to(device)"]},{"cell_type":"code","source":["model"],"metadata":{"id":"FZfUE7YRGBO8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":491141,"status":"ok","timestamp":1692147708099,"user":{"displayName":"이상범","userId":"04534362610835741286"},"user_tz":-540},"id":"uqAJKzoDYR8b","outputId":"ac1259cc-28f0-46a9-f18e-d0eedd6e1641"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/10] - Loss: 0.4309\n","Epoch [2/10] - Loss: 0.1712\n","Epoch [3/10] - Loss: 0.1706\n","Epoch [4/10] - Loss: 0.1689\n","Epoch [5/10] - Loss: 0.1679\n","Epoch [6/10] - Loss: 0.1660\n","Epoch [7/10] - Loss: 0.1656\n","Epoch [8/10] - Loss: 0.1653\n","Epoch [9/10] - Loss: 0.1641\n","Epoch [10/10] - Loss: 0.1636\n","Training finished.\n"]}],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","# 학습 루프\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for images, masks in train_loader:\n","        images, masks = images.to(device), masks.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, masks.squeeze().long())\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    average_loss = running_loss / len(train_loader)\n","    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {average_loss:.4f}\")\n","\n","print(\"Training finished.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27425,"status":"ok","timestamp":1692147735514,"user":{"displayName":"이상범","userId":"04534362610835741286"},"user_tz":-540},"id":"kKnsNqiqobmr","outputId":"8e989334-ea83-4f70-a6aa-05ada0915803"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n","Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n"]}],"source":["test_dataset = datasets.VOCSegmentation(root='./data', year='2012', image_set='val', transform=transform,  target_transform=transform, download=True)\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23P2wJopzswd"},"outputs":[],"source":["def convert_predictions_to_masks(predictions, threshold=0.5):\n","    # Assuming predictions are batch_size x num_classes x height x width\n","    batch_size, num_classes, height, width = predictions.size()\n","    binary_masks = (predictions > threshold).float()\n","    return binary_masks\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1BBM1ZSyI5qBvn5ZVsPoLCY2Gt1uOHyf7"},"id":"Orqs2P7gzg6d","outputId":"292a09a9-703e-4877-a8ff-e7136fa76136"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["for images, masks in test_loader:\n","    with torch.no_grad():\n","        # Forward pass to get the model's predictions\n","        images, masks = images.to(device), masks.to(device)\n","        predictions = model(images)\n","\n","        # Convert predictions to masks (assuming you have a function to do this)\n","        predicted_masks = convert_predictions_to_masks(predictions)\n","\n","        # Convert tensors to numpy arrays\n","        images_np = images.cpu().numpy().squeeze(0).transpose(1, 2, 0)\n","        masks_np = masks.cpu().numpy().squeeze(0)\n","        predicted_masks_np = predicted_masks.cpu().numpy()\n","\n","        # Plotting\n","        plt.figure(figsize=(12, 6))\n","        plt.subplot(1, 3, 1)\n","        plt.imshow(images_np)\n","        plt.title(\"Input Image\")\n","\n","        plt.subplot(1, 3, 2)\n","        plt.imshow(masks_np[0], cmap='jet', vmin=0, vmax=21 - 1)\n","        plt.title(\"Ground Truth Mask\")\n","\n","        plt.subplot(1, 3, 3)\n","\n","        plt.imshow(predicted_masks_np[0][1], cmap='jet', vmin=0, vmax=21 - 1)\n","        plt.title(\"Predicted Mask\")\n","\n","        plt.tight_layout()\n","        plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gTJD_Y_S0VwG"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyM3O+QZteTombFi11dR4MCU"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}